{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "\"They won't be very interesting, I'm afraid.\",\n",
    "\"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'], ['They', 'wo', \"n't\", 'be', 'very', 'interesting', ',', 'I', \"'m\", 'afraid', '.'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', '_learn', 'how', 'basic', 'text', 'cleaning', 'works_', 'on', '*very', 'simple*', 'data', '.']]\n"
     ]
    }
   ],
   "source": [
    "#NLTK makes it easy to convert documents-as-strings into word-vectors, \n",
    "#a process called tokenizing\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'wo', 'nt', 'be', 'very', 'interesting', 'I', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', 'learn', 'how', 'basic', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    \n",
    "    new_review = []\n",
    "    for token in review: \n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'simple', 'basic', 'sentences'], ['They', 'wo', 'nt', 'interesting', 'I', 'afraid'], ['The', 'point', 'examples', 'learn', 'basic', 'text', 'cleaning', 'works', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "            \n",
    "print(tokenized_docs_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'simple', 'basic', 'sentence'], ['They', 'wo', 'nt', 'interesting', 'I', 'afraid'], ['The', 'point', 'example', 'learn', 'basic', 'text', 'cleaning', 'work', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tokenized_docs_no_stopwords:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        #final_doc.append(porter.stem(word))\n",
    "        #final_doc.append(snowball.stem(word))\n",
    "        final_doc.append(wordnet.lemmatize(word)) #note that lemmatize() can also takes part of speech as an argument!\n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n",
    "print(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing HTML entities and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don't like the &quot;Chicken Soup for the Soul&quot; series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)\n",
      "<p>While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don't like the \"Chicken Soup for the Soul\" series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)\n"
     ]
    }
   ],
   "source": [
    "import re, html\n",
    "\n",
    "##\n",
    "# Removes HTML or XML character references and entities from a text string.\n",
    "#\n",
    "# @param text The HTML (or XML) source text.\n",
    "# @return The plain text, as a Unicode string, if necessary.\n",
    "# AUTHOR: Fredrik Lundh\n",
    "\n",
    "def unescape(text):\n",
    "    def fixup(m):\n",
    "        text = m.group(0)\n",
    "        if text[:2] == \"&#\":\n",
    "            # character reference\n",
    "            try:\n",
    "                if text[:3] == \"&#x\":\n",
    "                    return chr(int(text[3:-1], 16)) #16è¿›åˆ¶\n",
    "                else:\n",
    "                    return chr(int(text[2:-1]))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        else:\n",
    "            # named entity\n",
    "            try:\n",
    "                text = chr(html.entities.name2codepoint[text[1:-1]])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return text # leave as is\n",
    "    return re.sub(\"&#?\\w+;\", fixup, text)\n",
    "\n",
    "test_string =\"<p>While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don't like the &quot;Chicken Soup for the Soul&quot; series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)\"\n",
    "\n",
    "print(test_string)\n",
    "print(unescape(test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don\\'t like the \"Chicken Soup for the Soul\" series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "soup = bs(unescape(test_string), 'lxml')\n",
    "soup.get_text() #notice that it returns unicode!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tweeter thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tweet = 'I luv my &lt;3 iphone &amp; youâ€™re awsm apple. DisplayIsAwesome, sooo happppppy ðŸ™‚ http://www.apple.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.escape html character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_parser = html.parser.HTMLParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weiweizheng/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "tweet = html_parser.unescape(original_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I luv my <3 iphone & youâ€™re awsm apple. DisplayIsAwesome, sooo happppppy ðŸ™‚ http://www.apple.com'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.decoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'I luv my <3 iphone & youre awsm apple. DisplayIsAwesome, sooo happppppy  http://www.apple.com'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = tweet.encode().decode(\"utf8\").encode('ascii','ignore')\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Attached Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I luv my &lt;3 iphone &amp; youâ€™re awsm apple.  Display Is Awesome, sooo happppppy ðŸ™‚ http://www.apple.com'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = ' '.join(re.findall('[A-Z][^A-Z]*', original_tweet))\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#itertools.groupby() #https://docs.python.org/3/library/itertools.html#itertools.groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING TEXT FOR NATURAL LANGUAGE PROCESSING TASKS IN MACHINE LEARNING IN PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://ieva.rocks/2016/08/07/cleaning-text-for-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of three strings.\n",
    "incoming_reports = [\"We are attacking on their left flank but are losing many men.\", \n",
    "               \"We cannot see the enemy army. Nothing else to report.\", \n",
    "               \"We are ready to attack but are waiting for your orders.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['We',\n",
       "  'are',\n",
       "  'attacking',\n",
       "  'on',\n",
       "  'their',\n",
       "  'left',\n",
       "  'flank',\n",
       "  'but',\n",
       "  'are',\n",
       "  'losing',\n",
       "  'many',\n",
       "  'men',\n",
       "  '.'],\n",
       " ['We',\n",
       "  'can',\n",
       "  'not',\n",
       "  'see',\n",
       "  'the',\n",
       "  'enemy',\n",
       "  'army',\n",
       "  '.',\n",
       "  'Nothing',\n",
       "  'else',\n",
       "  'to',\n",
       "  'report',\n",
       "  '.'],\n",
       " ['We',\n",
       "  'are',\n",
       "  'ready',\n",
       "  'to',\n",
       "  'attack',\n",
       "  'but',\n",
       "  'are',\n",
       "  'waiting',\n",
       "  'for',\n",
       "  'your',\n",
       "  'orders',\n",
       "  '.']]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Apply word_tokenize to each element of the list called incoming_reports\n",
    "tokenized_reports = [word_tokenize(report) for report in incoming_reports]\n",
    "\n",
    "# View tokenized_reports\n",
    "tokenized_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['We',\n",
       "  'are',\n",
       "  'attacking',\n",
       "  'on',\n",
       "  'their',\n",
       "  'left',\n",
       "  'flank',\n",
       "  'but',\n",
       "  'are',\n",
       "  'losing',\n",
       "  'many',\n",
       "  'men'],\n",
       " ['We',\n",
       "  'can',\n",
       "  'not',\n",
       "  'see',\n",
       "  'the',\n",
       "  'enemy',\n",
       "  'army',\n",
       "  'Nothing',\n",
       "  'else',\n",
       "  'to',\n",
       "  'report'],\n",
       " ['We',\n",
       "  'are',\n",
       "  'ready',\n",
       "  'to',\n",
       "  'attack',\n",
       "  'but',\n",
       "  'are',\n",
       "  'waiting',\n",
       "  'for',\n",
       "  'your',\n",
       "  'orders']]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import regex\n",
    "import re\n",
    "\n",
    "# Import string\n",
    "import string\n",
    "\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
    "\n",
    "tokenized_reports_no_punctuation = []\n",
    "\n",
    "for review in tokenized_reports:\n",
    "    \n",
    "    new_review = []\n",
    "    for token in review: \n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_reports_no_punctuation.append(new_review)\n",
    "    \n",
    "tokenized_reports_no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['We', 'attacking', 'left', 'flank', 'losing', 'many', 'men'],\n",
       " ['We', 'see', 'enemy', 'army', 'Nothing', 'else', 'report'],\n",
       " ['We', 'ready', 'attack', 'waiting', 'orders']]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_reports_no_stopwords = []\n",
    "for report in tokenized_reports_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in report:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    tokenized_reports_no_stopwords.append(new_term_vector)\n",
    "            \n",
    "tokenized_reports_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
